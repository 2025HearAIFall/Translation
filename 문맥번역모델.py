# -*- coding: utf-8 -*-
"""문맥번역모델.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1il273iMm5vXyL3_gQ0XHORZsjXApc4-t
"""

# 충돌나는 패키지들을 먼저 제거
!pip -q uninstall -y transformers datasets accelerate numpy

# NumPy 1.x 기준으로 호환 버전 설치
!pip -q install "numpy<2.0" transformers==4.44.2 datasets==2.19.0 \
                accelerate==0.30.1 sentencepiece==0.2.0 \
                sacrebleu==2.4.0 rouge-score==0.1.2

# 0) (선택) 지금 충돌 유발 가능성이 있는 패키지들 확인
!pip list | egrep -i "numpy|opencv|albume|spacy|thinc|gcsfs|fsspec|jax|jaxlib|dopamine" || true

# 1) 우리 작업에 불필요하고 충돌을 유발하는 패키지들 제거
!pip -q uninstall -y opencv-python opencv-python-headless opencv-contrib-python \
  albumentations albucore spacy thinc gcsfs dopamine-rl jax jaxlib || true

# 2) 우리가 쓸 스택과 충돌할 수 있는 것들 제거
!pip -q uninstall -y transformers datasets accelerate numpy fsspec || true

# 3) 설치 기반 정리(캐시/업그레이드)
!pip -q install --no-cache-dir -U pip setuptools wheel

# 4) 먼저 핵심 하위 의존성부터 고정 (numpy, fsspec)
!pip -q install --no-cache-dir "numpy==1.26.4" "fsspec==2024.3.1"

# 5) 번역 파이프라인 패키지 설치 (서로 호환 검증된 조합)
!pip -q install --no-cache-dir transformers==4.44.2 datasets==2.19.0 \
  accelerate==0.30.1 sentencepiece==0.2.0 sacrebleu==2.4.0 rouge-score==0.1.2

# 6) 버전 확인
import numpy, transformers, datasets, accelerate, sentencepiece, sacrebleu, rouge_score, fsspec
print("numpy:", numpy.__version__)
print("transformers:", transformers.__version__)
print("datasets:", datasets.__version__)
print("accelerate:", accelerate.__version__)
print("sentencepiece:", sentencepiece.__version__)
print("sacrebleu:", sacrebleu.__version__)
print("rouge-score:", rouge_score.__version__)
print("fsspec:", fsspec.__version__)

from google.colab import drive
drive.mount('/content/drive')

# 방법 B: 드라이브의 폴더에 복사해 두었다면 경로만 지정
train_path = "/content/drive/MyDrive/train.jsonl"
test_path  = "/content/drive/MyDrive/test.jsonl"

from datasets import load_dataset

ds = load_dataset("json", data_files={"train": train_path, "validation": test_path})
ds

from transformers import AutoTokenizer

MODEL_NAME = "google/mt5-small"
tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, use_fast=True)

prefix = "fix: "
max_input_len = 96
max_target_len = 96

def preprocess(batch):
    inputs = [prefix + x for x in batch["src"]]
    model_inputs = tokenizer(inputs, max_length=max_input_len, truncation=True)
    with tokenizer.as_target_tokenizer():
        labels = tokenizer(batch["tgt"], max_length=max_target_len, truncation=True)
    model_inputs["labels"] = labels["input_ids"]
    return model_inputs

tokenized = ds.map(preprocess, batched=True, remove_columns=ds["train"].column_names)
tokenized

!pip install -q evaluate

import numpy as np
import evaluate

bleu = evaluate.load("sacrebleu")
rouge = evaluate.load("rouge")

def compute_metrics(eval_pred):
    preds, labels = eval_pred
    preds = np.where(preds != -100, preds, tokenizer.pad_token_id)
    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)
    pred_str = tokenizer.batch_decode(preds, skip_special_tokens=True)
    label_str = tokenizer.batch_decode(labels, skip_special_tokens=True)
    bleu_score = bleu.compute(predictions=pred_str, references=[[l] for l in label_str])["score"]
    rouge_l = rouge.compute(predictions=pred_str, references=label_str)["rougeL"]
    return {"sacrebleu": bleu_score, "rougeL": rouge_l}

# accelerate를 호환 버전으로 업그레이드
!pip -q install -U accelerate==0.33.0
# (문제 지속 시 최신 소폭 상향)
# !pip -q install -U accelerate==0.34.2

import torch
from transformers import AutoModelForSeq2SeqLM, DataCollatorForSeq2Seq, AutoTokenizer
from transformers import Seq2SeqTrainingArguments, Seq2SeqTrainer, TrainerCallback
from datasets import load_dataset
import numpy as np
import evaluate
import os

bleu = evaluate.load("sacrebleu")
rouge = evaluate.load("rouge")

def compute_metrics(eval_pred):
    preds, labels = eval_pred
    preds = np.where(preds != -100, preds, tokenizer.pad_token_id)
    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)
    pred_str = tokenizer.batch_decode(preds, skip_special_tokens=True)
    label_str = tokenizer.batch_decode(labels, skip_special_tokens=True)
    bleu_score = bleu.compute(predictions=pred_str, references=[[l] for l in label_str])["score"]
    rouge_l = rouge.compute(predictions=pred_str, references=label_str)["rougeL"]
    return {"sacrebleu": bleu_score, "rougeL": rouge_l}


MODEL_NAME = "google/mt5-small" # Define MODEL_NAME here
tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, use_fast=True)

# Load and preprocess the dataset
train_path = "/content/drive/MyDrive/train.jsonl"
test_path  = "/content/drive/MyDrive/test.jsonl"

ds = load_dataset("json", data_files={"train": train_path, "validation": test_path})

prefix = "fix: "
max_input_len = 96
max_target_len = 96

def preprocess(batch):
    inputs = [prefix + x for x in batch["src"]]
    model_inputs = tokenizer(inputs, max_length=max_input_len, truncation=True)
    with tokenizer.as_target_tokenizer():
        labels = tokenizer(batch["tgt"], max_length=max_target_len, truncation=True)
    model_inputs["labels"] = labels["input_ids"]
    return model_inputs

tokenized = ds.map(preprocess, batched=True, remove_columns=ds["train"].column_names)


model = AutoModelForSeq2SeqLM.from_pretrained(MODEL_NAME)
data_collator = DataCollatorForSeq2Seq(tokenizer, model=model)

# Explicitly make model parameters contiguous
for param in model.parameters():
    if not param.is_contiguous():
        param.data = param.data.contiguous()


training_args = Seq2SeqTrainingArguments(
    output_dir="/content/drive/MyDrive/context_trans_ckpt",
    evaluation_strategy="epoch",
    save_strategy="epoch",
    learning_rate=3e-4,              # 소량 데이터 → 약간 높게
    per_device_train_batch_size=16,
    per_device_eval_batch_size=16,
    num_train_epochs=8,              # 3~5 사이에서 시작 권장
    weight_decay=0.01,
    warmup_ratio=0.03,                # 추가
    label_smoothing_factor=0.1,       # 추가
    logging_steps=50,
    predict_with_generate=True,
    generation_max_length=128,
    fp16=torch.cuda.is_available(),
    save_total_limit=2,
    report_to="none",
)

# Removed the custom SaveStateDictCallback as we are trying to fix the contiguity issue before training

trainer = Seq2SeqTrainer(
    model=model,
    args=training_args,
    tokenizer=tokenizer,
    data_collator=data_collator,
    train_dataset=tokenized["train"],
    eval_dataset=tokenized["validation"],
    compute_metrics=compute_metrics,
    # Removed callbacks=[SaveStateDictCallback()]
)

trainer.train()

#모델저
save_dir = "/content/drive/MyDrive/context_trans_model_v1"
trainer.save_model(save_dir)
tokenizer.save_pretrained(save_dir)
print("Saved to", save_dir)

#추론(테스트 셋 몇개 확)
from transformers import AutoTokenizer, AutoModelForSeq2SeqLM

device = "cuda" if torch.cuda.is_available() else "cpu"
tok = AutoTokenizer.from_pretrained(save_dir)
mdl = AutoModelForSeq2SeqLM.from_pretrained(save_dir).to(device)

def rewrite(text, num_beams=4, max_len=96):
    inp = tok.encode("fix: " + text, return_tensors="pt", truncation=True, max_length=96).to(device)
    out = mdl.generate(inp, num_beams=num_beams, max_length=max_len, no_repeat_ngram_size=3)
    return tok.decode(out[0], skip_special_tokens=True)

# 테스트셋 상위 5개 비교 출력
raw_valid = ds["validation"]
for i in range(5):
    s, t = raw_valid[i]["src"], raw_valid[i]["tgt"]
    pred = rewrite(s)
    print(f"[{i}]")
    print("SRC :", s)
    print("PRED:", pred)
    print("TGT :", t)
    print("-"*60)

#테스트셋 로드
from datasets import load_dataset

ds_test = load_dataset("json", data_files={"test": test_path})["test"]
print(ds_test)
print("Examples:", ds_test[:2])

#모델/토크나이저 로드&배치추론
from transformers import AutoTokenizer, AutoModelForSeq2SeqLM
import torch
from tqdm.auto import tqdm
from datasets import load_dataset # Import load_dataset

prefix = "fix: " # Define prefix here
max_gen_len = 96 # Define max_gen_len here
num_beams = 4 # Define num_beams here
batch_size = 16 # Define batch_size here
save_dir = "/content/drive/MyDrive/context_trans_model_v1" # Define save_dir here

device = "cuda" if torch.cuda.is_available() else "cpu" # Define device here

# Load the test dataset
test_path  = "/content/drive/MyDrive/test.jsonl" # Define test_path
ds_test = load_dataset("json", data_files={"test": test_path})["test"] # Define ds_test


tok = AutoTokenizer.from_pretrained(save_dir, use_fast=True)
mdl = AutoModelForSeq2SeqLM.from_pretrained(save_dir).to(device)
mdl.eval()

def generate_batch_texts(batch_src):
    inputs = tok([prefix + s for s in batch_src],
                 return_tensors="pt",
                 padding=True,
                 truncation=True,
                 max_length=max_gen_len).to(device)
    with torch.no_grad():
        out = mdl.generate(**inputs,
                           num_beams=num_beams,
                           max_length=max_gen_len,
                           no_repeat_ngram_size=3)
    return tok.batch_decode(out, skip_special_tokens=True)

preds = []
for i in tqdm(range(0, len(ds_test), batch_size)):
    batch = ds_test[i:i+batch_size]
    batch_src = batch["src"]
    batch_pred = generate_batch_texts(batch_src)
    preds.extend(batch_pred)

refs = ds_test["tgt"]
assert len(preds) == len(refs)
print("Sample\nSRC:", ds_test[0]["src"], "\nPRED:", preds[0], "\nTGT:", refs[0])

#3) 표준 번역 지표 (SacreBLEU / chrF++ / ROUGE-L / TER)
import evaluate

metric_bleu = evaluate.load("sacrebleu")
metric_chrf = evaluate.load("chrf")
metric_rouge = evaluate.load("rouge")
metric_ter  = evaluate.load("ter")

# BLEU/TER은 ref를 리스트의 리스트로 받습니다.
bleu = metric_bleu.compute(predictions=preds, references=[[r] for r in refs])["score"]
chrf = metric_chrf.compute(predictions=preds, references=refs)["score"]
rougeL = metric_rouge.compute(predictions=preds, references=refs)["rougeL"]
ter = metric_ter.compute(predictions=preds, references=refs)["score"]

print(f"SacreBLEU : {bleu:.2f}")
print(f"chrF++    : {chrf:.2f}")
print(f"ROUGE-L   : {rougeL:.4f}")
print(f"TER       : {ter:.2f} (낮을수록 좋음)")

#추가 진단 지표 (정확일치/길이/다양도)
import numpy as np

# 정확히 동일한 문장 비율(엄격)
exact_match = np.mean([int(p.strip()==r.strip()) for p, r in zip(preds, refs)])

# 길이 비율(예측/정답), 길이가 너무 늘어나거나 줄어드는지 확인
len_pred = np.array([len(p) for p in preds])
len_ref  = np.array([len(r) for r in refs])
len_ratio = (len_pred / np.maximum(1, len_ref)).mean()

# distinct-1/2 (uni/bi-gram의 고유 비율 → 반복/복붙 경향 확인)
def distinct_ngram_ratio(texts, n=1):
    total = 0
    uniq = 0
    for t in texts:
        tokens = t.split()
        ngrams = [' '.join(tokens[i:i+n]) for i in range(0, max(0, len(tokens)-n+1))]
        total += len(ngrams)
        uniq  += len(set(ngrams))
    return (uniq / total) if total>0 else 0.0

distinct1 = distinct_ngram_ratio(preds, n=1)
distinct2 = distinct_ngram_ratio(preds, n=2)

print(f"Exact Match      : {exact_match*100:.2f}%")
print(f"Length Ratio     : {len_ratio:.3f}  (예측/정답, 1.0에 가까울수록 적절)")
print(f"Distinct-1       : {distinct1:.3f}")
print(f"Distinct-2       : {distinct2:.3f}")

#길이 구간별(Bucket) 성능 분석
import pandas as pd

df = pd.DataFrame({
    "src": ds_test["src"],
    "tgt": refs,
    "pred": preds,
    "len_tgt": [len(r) for r in refs],
})

# 길이에 따라 5개 구간으로 나눔(원하면 10개도 가능)
df["len_bucket"] = pd.qcut(df["len_tgt"], q=5, duplicates="drop")

bucket_rows = []
for bucket, sub in df.groupby("len_bucket"):
    sb_preds = sub["pred"].tolist()
    sb_refs  = sub["tgt"].tolist()
    b_bleu = metric_bleu.compute(predictions=sb_preds, references=[[r] for r in sb_refs])["score"]
    b_chrf = metric_chrf.compute(predictions=sb_preds, references=sb_refs)["score"]
    b_rougeL = metric_rouge.compute(predictions=sb_preds, references=sb_refs)["rougeL"]
    b_ter  = metric_ter.compute(predictions=sb_preds, references=sb_refs)["score"]
    bucket_rows.append([str(bucket), len(sub), b_bleu, b_chrf, b_rougeL, b_ter])

df_bucket = pd.DataFrame(bucket_rows, columns=["len_bucket","n","BLEU","chrF++","ROUGE-L","TER"]).sort_values("len_bucket")
df_bucket

# 간단히 "문자 레벨" 차이를 기준으로 한 유사도 점수(0~1) → 낮을수록 차이 큼 (빠른 예비 확인용)
import difflib

def quick_similarity(a, b):
    return difflib.SequenceMatcher(None, a, b).ratio()

df["sim"] = [quick_similarity(p, r) for p, r in zip(df["pred"], df["tgt"])]
df_bad = df.sort_values("sim").head(20)[["src","pred","tgt","sim","len_tgt"]]
df_bad.reset_index(drop=True, inplace=True)
df_bad.head(10)

out_dir = "/content/drive/MyDrive/context_eval_v1"
import os, json
os.makedirs(out_dir, exist_ok=True)

# 전체 예측 테이블
df.to_csv(f"{out_dir}/predictions.csv", index=False)

# 요약 메트릭 저장
summary = {
    "SacreBLEU": round(bleu, 2),
    "chrF++": round(chrf, 2),
    "ROUGE-L": round(rougeL, 4),
    "TER": round(ter, 2),
    "ExactMatch": round(float(exact_match), 4),
    "LengthRatio": round(float(len_ratio), 4),
    "Distinct1": round(float(distinct1), 4),
    "Distinct2": round(float(distinct2), 4),
}
with open(f"{out_dir}/metrics.json", "w", encoding="utf-8") as f:
    json.dump(summary, f, ensure_ascii=False, indent=2)

# 길이구간 리포트
df_bucket.to_csv(f"{out_dir}/bucket_report.csv", index=False)

print("Saved:\n", out_dir)
summary



